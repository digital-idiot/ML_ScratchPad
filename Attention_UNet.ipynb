{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from pathlib import Path\n",
    "from math import floor, ceil\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from rasterio import windows as rio_windows\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from advanced_losses import DiceLossVariants\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.layers import Activation, add, multiply\n",
    "from tensorflow.keras.layers import MaxPooling2D, SpatialDropout2D\n",
    "from tensorflow.keras.layers import UpSampling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "droprate = 0.3\n",
    "image_height = 6000\n",
    "image_width = 6000\n",
    "window_height = 512\n",
    "window_width = 512\n",
    "min_height_overlap = 32\n",
    "min_width_overlap = 32\n",
    "boundless_flag = True\n",
    "class_count = 6\n",
    "data_shuffle = True\n",
    "batchsize=2\n",
    "bands = (1, 2, 3, 4, 5)\n",
    "image_features = len(bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_augs = (\n",
    "    # ((lambda m : m), True),\n",
    "    (partial(np.rot90, k=1, axes=(1, 2)), True),\n",
    "    (partial(np.rot90, k=2, axes=(1, 2)), True),\n",
    "    (partial(np.rot90, k=3, axes=(1, 2)), True),\n",
    "    (partial(np.flip, axis=1), True),\n",
    "    (partial(np.flip, axis=2), True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data\n",
    "config_dir = Path('Configs')\n",
    "train_config = config_dir / 'Train_Map.json'\n",
    "valid_config = config_dir / 'Validation_Map.json'\n",
    "test_config = config_dir / 'Test_Map.json'\n",
    "model_dir = Path(\"Models\")\n",
    "mplot = model_dir / \"Model_Plot.png\"\n",
    "model_max_accuracy = model_dir / 'Model_MaxAccuracy.h5' \n",
    "model_min_loss = model_dir / 'Model_MinLoss.h5'\n",
    "log_d = Path('Logs')\n",
    "\n",
    "with open(train_config.as_posix(), 'r') as tm:\n",
    "    train_map = json.load(tm)\n",
    "\n",
    "with open(valid_config.as_posix(), 'r') as tm:\n",
    "    valid_map = json.load(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_windows(img_height, img_width, win_height, win_width, min_hoverlap, min_woverlap, boundless=False):\n",
    "    hc = ceil((img_height - min_hoverlap) / (win_height - min_hoverlap))\n",
    "    wc = ceil((img_width - min_woverlap) / (win_width - min_woverlap))\n",
    "    \n",
    "    \n",
    "    h_overlap = ((hc * win_height) - img_height) // (hc - 1)\n",
    "    w_overlap = ((wc * win_height) - img_width) // (wc - 1)\n",
    "    \n",
    "    \n",
    "    hslack_res = ((hc * win_height) - img_height) % (hc - 1)\n",
    "    wslack_res = ((wc * win_width) - img_width) % (wc - 1)\n",
    "    \n",
    "    dh = win_height - h_overlap\n",
    "    dw = win_width - w_overlap\n",
    "    \n",
    "    row_offsets = np.arange(0, (img_height-h_overlap), dh)\n",
    "    col_offsets = np.arange(0, (img_width-w_overlap), dw)\n",
    "    \n",
    "    if hslack_res > 0:\n",
    "        row_offsets[-hslack_res:] -= np.arange(1, (hslack_res + 1), 1)\n",
    "    if wslack_res > 0:\n",
    "        col_offsets[-wslack_res:] -= np.arange(1, (wslack_res + 1), 1)\n",
    "    \n",
    "    row_offsets = row_offsets.tolist()\n",
    "    col_offsets = col_offsets.tolist()\n",
    "    \n",
    "    offsets = product(col_offsets, row_offsets)\n",
    "    \n",
    "    indices = product(range(len(col_offsets)), range(len(row_offsets)))\n",
    "    \n",
    "    big_window = rio_windows.Window(col_off=0, row_off=0, width=img_width, height=img_height)\n",
    "    \n",
    "    for index, (col_off, row_off) in zip(indices, offsets):\n",
    "        window = rio_windows.Window(\n",
    "            col_off=col_off,\n",
    "            row_off=row_off,\n",
    "            width=win_width,\n",
    "            height=win_height\n",
    "        )\n",
    "        if boundless:\n",
    "            yield index, window\n",
    "        else:\n",
    "            yield index, window.intersection(big_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterDataGenerator(Sequence):\n",
    "    def __init__(\n",
    "        self,  \n",
    "        map_dict,\n",
    "        channels,\n",
    "        img_height,\n",
    "        img_width,\n",
    "        win_height,\n",
    "        win_width,\n",
    "        min_hoverlap,\n",
    "        min_woverlap,\n",
    "        cls_count,\n",
    "        augs=None, # list of tuples like (fn, flag), fn: aug function works on channel first image, flag: wheather fn should be applied on labels\n",
    "        boundless=False,\n",
    "        shuffle=True,\n",
    "        batch_size=1,\n",
    "    ):\n",
    "        assert isinstance(map_dict, dict), 'Invalid type for parameter <map_dict>, expected type `dict`!'\n",
    "        assert all([set(map_dict[k].keys()) == {'IMAGE', 'LABEL'} for k in map_dict.keys()]), \"Invalid map <dict_map>, Key Mismatch!\"\n",
    "        if augs is None:\n",
    "            augs = (((lambda m : m), True),)\n",
    "        else:\n",
    "            assert isinstance(augs, (tuple, list)) and all(\n",
    "                [callable(fn) and isinstance(flag, bool) for (fn, flag) in augs]\n",
    "            )\n",
    "        \n",
    "        couples =  [(Path(couple['IMAGE']).as_posix(), Path(couple['LABEL']).as_posix()) for couple in map_dict.values()]\n",
    "        \n",
    "        windows = list(\n",
    "            generate_windows(\n",
    "                img_height=img_height,\n",
    "                img_width=img_width,\n",
    "                win_height=win_height,\n",
    "                win_width=win_width,\n",
    "                min_hoverlap=min_hoverlap,\n",
    "                min_woverlap=min_woverlap,\n",
    "                boundless=boundless\n",
    "            )\n",
    "        )\n",
    "        dat = list(product(couples, windows, augs))\n",
    "        if shuffle:\n",
    "            random.shuffle(dat)\n",
    "        self.data = dat\n",
    "        self.channels = channels\n",
    "        self.class_count = cls_count\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        current_batch = self.data[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        islices = list()\n",
    "        lslices = list()\n",
    "        for (im, lb), (_, w), (aug, af) in current_batch:\n",
    "            with rio.open(im, 'r') as isrc:\n",
    "                islice = isrc.read(indexes=self.channels, window=w, boundless=boundless_flag, masked=False)\n",
    "                islice = aug(islice)\n",
    "                islice = np.moveaxis(a=islice, source=0, destination=-1)\n",
    "                islices.append(islice)\n",
    "            with rio.open(lb, 'r') as lsrc:\n",
    "                lslice = lsrc.read(window=w, boundless=boundless_flag, masked=False)\n",
    "                if af is True:\n",
    "                    lslice = aug(lslice)\n",
    "                lslice = np.moveaxis(a=lslice, source=0, destination=-1)\n",
    "                lslice =to_categorical(\n",
    "                    y=(lslice-1), \n",
    "                    num_classes=self.class_count\n",
    "                )\n",
    "                lslices.append(lslice)\n",
    "        ibatch = np.stack(islices, axis=0)\n",
    "        lbatch = np.stack(lslices, axis=0)\n",
    "        return ibatch, lbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_conv(filters, kernel_size, strides, padding):\n",
    "    return Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n",
    "\n",
    "\n",
    "def upsample_simple(filters, kernel_size, strides, padding):\n",
    "    return UpSampling2D(filters, kernel_size, strides=strides, padding=padding)\n",
    "\n",
    "\n",
    "def attention_gate(inp_1, inp_2, n_intermediate_filters, k_init='he_normal'):\n",
    "    \"\"\"Attention gate. Compresses both inputs to n_intermediate_filters filters before processing.\n",
    "       Implemented as proposed by Oktay et al. in their Attention U-net, see: https://arxiv.org/abs/1804.03999.\n",
    "    \"\"\"\n",
    "    inp_1_conv = Conv2D(\n",
    "        n_intermediate_filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=k_init,\n",
    "    )(inp_1)\n",
    "    inp_2_conv = Conv2D(\n",
    "        n_intermediate_filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=k_init,\n",
    "    )(inp_2)\n",
    "\n",
    "    f = Activation(\"relu\")(add([inp_1_conv, inp_2_conv]))\n",
    "    g = Conv2D(\n",
    "        filters=1,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=k_init,\n",
    "    )(f)\n",
    "    h = Activation(\"sigmoid\")(g)\n",
    "    return multiply([inp_1, h])\n",
    "\n",
    "\n",
    "def attention_concat(conv_below, skip_connection):\n",
    "    \"\"\"Performs concatenation of upsampled conv_below with attention gated version of skip-connection\n",
    "    \"\"\"\n",
    "    below_filters = conv_below.get_shape().as_list()[-1]\n",
    "    attention_across = attention_gate(skip_connection, conv_below, below_filters)\n",
    "    return concatenate([conv_below, attention_across])\n",
    "\n",
    "\n",
    "def conv2d_block(\n",
    "    inputs,\n",
    "    use_batch_norm=True,\n",
    "    dropout=0.3,\n",
    "    dropout_type=\"spatial\",\n",
    "    filters=16,\n",
    "    kernel_size=(3, 3),\n",
    "    activation=\"relu\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    padding=\"same\",\n",
    "    momentum= 0.95\n",
    "):\n",
    "\n",
    "    if dropout_type == \"spatial\":\n",
    "        DO = SpatialDropout2D\n",
    "    elif dropout_type == \"standard\":\n",
    "        DO = Dropout\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"dropout_type must be one of ['spatial', 'standard'], got {dropout_type}\"\n",
    "        )\n",
    "    if isinstance(activation, str):\n",
    "        c = Conv2D(\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            activation=activation,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            padding=padding,\n",
    "            use_bias=not use_batch_norm,\n",
    "        )(inputs)\n",
    "    else:\n",
    "        assert 'keras.layers' in getattr(activation, '__module__', None) and isinstance(activation, type(activation))\n",
    "        c = activation(\n",
    "            Conv2D(\n",
    "                filters,\n",
    "                kernel_size,\n",
    "                activation=activation,\n",
    "                kernel_initializer=kernel_initializer,\n",
    "                padding=padding,\n",
    "                use_bias=not use_batch_norm,\n",
    "            )(inputs)\n",
    "        )\n",
    "        \n",
    "    if use_batch_norm:\n",
    "        c = BatchNormalization()(c)\n",
    "    if dropout > 0.0:\n",
    "        c = DO(dropout)(c)\n",
    "    c = Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        activation=activation,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=not use_batch_norm,\n",
    "    )(c)\n",
    "    if use_batch_norm:\n",
    "        c = BatchNormalization(momentum=momentum)(c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def xnet(\n",
    "    input_shape=(None, None, image_features),\n",
    "    num_classes=class_count,\n",
    "    activation=\"relu\",\n",
    "    use_batch_norm=True,\n",
    "    upsample_mode=\"deconv\",  # 'deconv' or 'simple'\n",
    "    dropout=droprate,\n",
    "    dropout_change_per_layer=0.0,\n",
    "    dropout_type=\"spatial\",\n",
    "    use_dropout_on_upsampling=False,\n",
    "    use_attention=True,\n",
    "    filters=32,\n",
    "    num_layers=4,\n",
    "    output_activation=\"softmax\",\n",
    "):\n",
    "\n",
    "    if upsample_mode == \"deconv\":\n",
    "        upsample = upsample_conv\n",
    "    else:\n",
    "        upsample = upsample_simple\n",
    "\n",
    "    # Build U-Net model\n",
    "    inputs = Input(input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    down_layers = []\n",
    "    for l in range(num_layers):\n",
    "        x = conv2d_block(\n",
    "            inputs=x,\n",
    "            filters=filters,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            dropout=dropout,\n",
    "            dropout_type=dropout_type,\n",
    "            activation=activation,\n",
    "        )\n",
    "        down_layers.append(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        dropout += dropout_change_per_layer\n",
    "        filters = filters * 2  # double the number of filters with each layer\n",
    "\n",
    "    x = conv2d_block(\n",
    "        inputs=x,\n",
    "        filters=filters,\n",
    "        use_batch_norm=use_batch_norm,\n",
    "        dropout=dropout,\n",
    "        dropout_type=dropout_type,\n",
    "        activation=activation,\n",
    "    )\n",
    "\n",
    "    if not use_dropout_on_upsampling:\n",
    "        dropout = 0.3\n",
    "        dropout_change_per_layer = 0.0\n",
    "\n",
    "    for conv in reversed(down_layers):\n",
    "        filters //= 2  # decreasing number of filters with each layer\n",
    "        dropout -= dropout_change_per_layer\n",
    "        x = upsample(filters, (2, 2), strides=(2, 2), padding=\"same\")(x)\n",
    "        if use_attention:\n",
    "            x = attention_concat(conv_below=x, skip_connection=conv)\n",
    "        else:\n",
    "            x = concatenate([x, conv])\n",
    "        x = conv2d_block(\n",
    "            inputs=x,\n",
    "            filters=filters,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            dropout=dropout,\n",
    "            dropout_type=dropout_type,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation=output_activation)(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xnet_classifier = xnet(\n",
    "    input_shape=(None, None, image_features),\n",
    "    num_classes=class_count,\n",
    "    activation=\"relu\",\n",
    "    use_batch_norm=True,\n",
    "    upsample_mode=\"deconv\",  # 'deconv' or 'simple'\n",
    "    dropout=droprate,\n",
    "    dropout_change_per_layer=0.0,\n",
    "    dropout_type=\"standard\",\n",
    "    use_dropout_on_upsampling=False,\n",
    "    use_attention=True,\n",
    "    filters=64,\n",
    "    num_layers=4,\n",
    "    output_activation=\"softmax\",\n",
    ")\n",
    "xnet_classifier.summary(line_length=116)\n",
    "# plot_model(xnet_classifier, to_file=mplot, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_val_loss = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n",
    "# es_val_accu = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.001)\n",
    "mc_val_accu = ModelCheckpoint(str(model_max_accuracy.absolute()), monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "mc_val_loss = ModelCheckpoint(str(model_min_loss.absolute()), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "tb = TensorBoard(\n",
    "    log_dir=log_d, \n",
    "    histogram_freq=1, \n",
    "    write_graph=True, \n",
    "    write_images=True,\n",
    "    update_freq='batch', \n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "# Load Pre Trained Weights if any\n",
    "if model_max_accuracy.is_file():\n",
    "    xnet_classifier.load_weights(str(model_max_accuracy))\n",
    "\n",
    "xnet_classifier.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4), \n",
    "    # loss='categorical_crossentropy', \n",
    "    loss=DiceLossVariants(name='tversky'),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = RasterDataGenerator( \n",
    "    map_dict=train_map,\n",
    "    channels=bands,\n",
    "    img_height=image_height,\n",
    "    img_width=image_width,\n",
    "    win_height=window_height,\n",
    "    win_width=window_width,\n",
    "    min_hoverlap=min_height_overlap,\n",
    "    min_woverlap=min_width_overlap,\n",
    "    cls_count=class_count,\n",
    "    boundless=boundless_flag,\n",
    "    augs=geo_augs,\n",
    "    # augs=None,\n",
    "    shuffle=data_shuffle,\n",
    "    batch_size=batchsize\n",
    ")\n",
    "valid_generator = RasterDataGenerator(\n",
    "    map_dict=valid_map,\n",
    "    channels=bands,\n",
    "    img_height=image_height,\n",
    "    img_width=image_width,\n",
    "    win_height=window_height,\n",
    "    win_width=window_width,\n",
    "    min_hoverlap=1,\n",
    "    min_woverlap=1,\n",
    "    cls_count=class_count,\n",
    "    augs=None,\n",
    "    boundless=boundless_flag,\n",
    "    shuffle=data_shuffle,\n",
    "    batch_size=batchsize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "xnet_classifier.fit(\n",
    "    x=train_generator, \n",
    "    epochs=50, \n",
    "    validation_data=valid_generator,\n",
    "    use_multiprocessing=False,\n",
    "    callbacks=[\n",
    "        tb,\n",
    "        es_val_loss,\n",
    "        mc_val_loss,\n",
    "#         es_val_accu,\n",
    "        mc_val_accu,\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
