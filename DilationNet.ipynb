{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from pathlib import Path\n",
    "from math import floor, ceil\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from keras.models import Model\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from rasterio import windows as rio_windows\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers import Activation, add, multiply\n",
    "from keras.layers import MaxPooling2D, SpatialDropout2D\n",
    "from keras.layers import UpSampling2D, BatchNormalization\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Concatenate, Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as kb\n",
    "from keras.backend import int_shape\n",
    "\n",
    "def tversky_index(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        alpha: float = 0.5,\n",
    "        beta: float = 0.5,\n",
    "        eps: float = 1e-10,\n",
    "        preserve_axis=(0, -1)\n",
    "):\n",
    "    \"\"\"\n",
    "    Ref A: https://arxiv.org/abs/1706.05721\n",
    "    alpha = beta = 0.5 : Dice coefficient\n",
    "    alpha = beta = 1   : Tanimoto coefficient (also known as Jaccard Index)\n",
    "    alpha + beta = 1   : Produces set of F*-scores\n",
    "\n",
    "    Ref B: https://arxiv.org/abs/1707.03237\n",
    "    The scores should be computed for each voxel in a batch and for each\n",
    "    class separately. Thus for a 4D tensor the resultant scores should be a\n",
    "    2D tensor having the batch axis and label axis. Therefore for a typical\n",
    "    channels last 4D tensor the axis 0 and axis -1 should be preserved (See\n",
    "    default value for `preserve_axis` parameter)\n",
    "\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :param alpha:\n",
    "    :param beta:\n",
    "    :param eps:\n",
    "    :param preserve_axis:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # assert int_shape(y_true) == int_shape(y_pred), \"Shape Mismatch\"\n",
    "\n",
    "    once = kb.ones(kb.shape(y_true))\n",
    "    p0 = y_pred  # probability that voxels are class i\n",
    "    p1 = once - y_pred  # probability that voxels are not class i\n",
    "    g0 = y_true\n",
    "    g1 = once - y_true\n",
    "\n",
    "    dims = list(range(kb.ndim(p0)))\n",
    "    if isinstance(preserve_axis, int):\n",
    "        preserve_axis = (preserve_axis,)\n",
    "    assert isinstance(\n",
    "        preserve_axis, (tuple, list)\n",
    "    ) and all(\n",
    "        [\n",
    "            (isinstance(n, int) and ((0 <= n < kb.ndim(p0)) or (-kb.ndim(p0) <= n < 0)))\n",
    "            for n in preserve_axis\n",
    "         ]\n",
    "    ), '`preserve_axis`: Illegal value!'\n",
    "    preserve_axis = list(set(preserve_axis))\n",
    "    for ax in preserve_axis:\n",
    "        del dims[ax]\n",
    "\n",
    "    numerator = kb.sum(\n",
    "        x=p0 * g0,\n",
    "        axis=dims\n",
    "    ) + eps\n",
    "    denominator = numerator + alpha * kb.sum(\n",
    "        x=p0 * g1,\n",
    "        axis=dims\n",
    "    ) + beta * kb.sum(\n",
    "        x=p1 * g0,\n",
    "        axis=dims\n",
    "    ) + eps\n",
    "\n",
    "    t = numerator / denominator\n",
    "    return t\n",
    "\n",
    "def tversky_loss(\n",
    "        alpha: float = 0.5,\n",
    "        beta: float = 0.5,\n",
    "        eps: float = 1e-10,\n",
    "        along_axis=(0, -1),\n",
    "        norm=True\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param alpha:\n",
    "    :param beta:\n",
    "    :param eps:\n",
    "    :param along_axis:\n",
    "    :param norm\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def tversky_loss_function(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "    ):\n",
    "        t_values = tversky_index(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            eps=eps,\n",
    "            preserve_axis=along_axis\n",
    "        )\n",
    "        losses = kb.ones_like(x=t_values, dtype=t_values.dtype) - t_values\n",
    "        if norm:\n",
    "            return kb.mean(x=loses, axis=None, keepdims=False)\n",
    "        else:\n",
    "            agg_loss = kb.sum(x=losses, axis=None, keepdims=False)\n",
    "            return agg_loss\n",
    "    return tversky_loss_function\n",
    "\n",
    "def focal_tversky_loss(\n",
    "    alpha: float = 0.5,\n",
    "    beta: float = 0.5,\n",
    "    gamma=0.75,\n",
    "    eps: float = 1e-10,\n",
    "    along_axis=(0, -1),\n",
    "    norm=True\n",
    "):\n",
    "    def focal_tversky_loss_function(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "    ):\n",
    "        t_values = tversky_index(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            eps=eps,\n",
    "            preserve_axis=along_axis\n",
    "        )\n",
    "        losses = kb.pow((kb.ones_like(x=t_values, dtype=t_values.dtype) - t_values), gamma)\n",
    "        if norm:\n",
    "            return kb.mean(x=losses, axis=None, keepdims=False)\n",
    "        else:\n",
    "            agg_loss = kb.sum(x=losses, axis=None, keepdims=False)\n",
    "            return agg_loss\n",
    "    return focal_tversky_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "droprate = 0.3\n",
    "image_height = 6000\n",
    "image_width = 6000\n",
    "window_height = 512\n",
    "window_width = 512\n",
    "min_height_overlap = 32\n",
    "min_width_overlap = 32\n",
    "boundless_flag = True\n",
    "class_count = 6\n",
    "data_shuffle = True\n",
    "batchsize=1\n",
    "bands = (1, 2, 3, 4, 5)\n",
    "image_features = len(bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_augs = (\n",
    "    # ((lambda m : m), True),\n",
    "    (partial(np.rot90, k=1, axes=(1, 2)), True),\n",
    "    (partial(np.rot90, k=2, axes=(1, 2)), True),\n",
    "    (partial(np.rot90, k=3, axes=(1, 2)), True),\n",
    "    (partial(np.flip, axis=1), True),\n",
    "    (partial(np.flip, axis=2), True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data\n",
    "config_dir = Path('Configs')\n",
    "train_config = config_dir / 'Train_Map.json'\n",
    "valid_config = config_dir / 'Validation_Map.json'\n",
    "test_config = config_dir / 'Test_Map.json'\n",
    "model_dir = Path(\"Models\")\n",
    "mplot = model_dir / \"Model_Plot.png\"\n",
    "model_max_accuracy = model_dir / 'Model_MaxAccuracy.h5' \n",
    "model_min_loss = model_dir / 'Model_MinLoss.h5'\n",
    "log_d = Path('Logs')\n",
    "\n",
    "with open(train_config.as_posix(), 'r') as tm:\n",
    "    train_map = json.load(tm)\n",
    "\n",
    "with open(valid_config.as_posix(), 'r') as tm:\n",
    "    valid_map = json.load(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_windows(img_height, img_width, win_height, win_width, min_hoverlap, min_woverlap, boundless=False):\n",
    "    hc = ceil((img_height - min_hoverlap) / (win_height - min_hoverlap))\n",
    "    wc = ceil((img_width - min_woverlap) / (win_width - min_woverlap))\n",
    "    \n",
    "    \n",
    "    h_overlap = ((hc * win_height) - img_height) // (hc - 1)\n",
    "    w_overlap = ((wc * win_height) - img_width) // (wc - 1)\n",
    "    \n",
    "    \n",
    "    hslack_res = ((hc * win_height) - img_height) % (hc - 1)\n",
    "    wslack_res = ((wc * win_width) - img_width) % (wc - 1)\n",
    "    \n",
    "    dh = win_height - h_overlap\n",
    "    dw = win_width - w_overlap\n",
    "    \n",
    "    row_offsets = np.arange(0, (img_height-h_overlap), dh)\n",
    "    col_offsets = np.arange(0, (img_width-w_overlap), dw)\n",
    "    \n",
    "    if hslack_res > 0:\n",
    "        row_offsets[-hslack_res:] -= np.arange(1, (hslack_res + 1), 1)\n",
    "    if wslack_res > 0:\n",
    "        col_offsets[-wslack_res:] -= np.arange(1, (wslack_res + 1), 1)\n",
    "    \n",
    "    row_offsets = row_offsets.tolist()\n",
    "    col_offsets = col_offsets.tolist()\n",
    "    \n",
    "    offsets = product(col_offsets, row_offsets)\n",
    "    \n",
    "    indices = product(range(len(col_offsets)), range(len(row_offsets)))\n",
    "    \n",
    "    big_window = rio_windows.Window(col_off=0, row_off=0, width=img_width, height=img_height)\n",
    "    \n",
    "    for index, (col_off, row_off) in zip(indices, offsets):\n",
    "        window = rio_windows.Window(\n",
    "            col_off=col_off,\n",
    "            row_off=row_off,\n",
    "            width=win_width,\n",
    "            height=win_height\n",
    "        )\n",
    "        if boundless:\n",
    "            yield index, window\n",
    "        else:\n",
    "            yield index, window.intersection(big_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterDataGenerator(Sequence):\n",
    "    def __init__(\n",
    "        self,  \n",
    "        map_dict,\n",
    "        channels,\n",
    "        img_height,\n",
    "        img_width,\n",
    "        win_height,\n",
    "        win_width,\n",
    "        min_hoverlap,\n",
    "        min_woverlap,\n",
    "        cls_count,\n",
    "        augs=None, # list of tuples like (fn, flag), fn: aug function works on channel first image, flag: wheather fn should be applied on labels\n",
    "        boundless=False,\n",
    "        shuffle=True,\n",
    "        batch_size=1,\n",
    "    ):\n",
    "        assert isinstance(map_dict, dict), 'Invalid type for parameter <map_dict>, expected type `dict`!'\n",
    "        assert all([set(map_dict[k].keys()) == {'IMAGE', 'LABEL'} for k in map_dict.keys()]), \"Invalid map <dict_map>, Key Mismatch!\"\n",
    "        if augs is None:\n",
    "            augs = (((lambda m : m), True),)\n",
    "        else:\n",
    "            assert isinstance(augs, (tuple, list)) and all(\n",
    "                [callable(fn) and isinstance(flag, bool) for (fn, flag) in augs]\n",
    "            )\n",
    "        \n",
    "        couples =  [(Path(couple['IMAGE']).as_posix(), Path(couple['LABEL']).as_posix()) for couple in map_dict.values()]\n",
    "        \n",
    "        windows = list(\n",
    "            generate_windows(\n",
    "                img_height=img_height,\n",
    "                img_width=img_width,\n",
    "                win_height=win_height,\n",
    "                win_width=win_width,\n",
    "                min_hoverlap=min_hoverlap,\n",
    "                min_woverlap=min_woverlap,\n",
    "                boundless=boundless\n",
    "            )\n",
    "        )\n",
    "        dat = list(product(couples, windows, augs))\n",
    "        if shuffle:\n",
    "            random.shuffle(dat)\n",
    "        self.data = dat\n",
    "        self.channels = channels\n",
    "        self.class_count = cls_count\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        current_batch = self.data[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        islices = list()\n",
    "        lslices = list()\n",
    "        for (im, lb), (_, w), (aug, af) in current_batch:\n",
    "            with rio.open(im, 'r') as isrc:\n",
    "                islice = isrc.read(indexes=self.channels, window=w, boundless=boundless_flag, masked=False)\n",
    "                islice = aug(islice)\n",
    "                islice = np.moveaxis(a=islice, source=0, destination=-1)\n",
    "                islices.append(islice)\n",
    "            with rio.open(lb, 'r') as lsrc:\n",
    "                lslice = lsrc.read(window=w, boundless=boundless_flag, masked=False)\n",
    "                if af is True:\n",
    "                    lslice = aug(lslice)\n",
    "                lslice = np.moveaxis(a=lslice, source=0, destination=-1)\n",
    "                lslice =to_categorical(\n",
    "                    y=(lslice-1), \n",
    "                    num_classes=self.class_count\n",
    "                )\n",
    "                lslices.append(lslice)\n",
    "        ibatch = np.stack(islices, axis=0)\n",
    "        lbatch = np.stack(lslices, axis=0)\n",
    "        return ibatch, lbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(input_shape, f=128):\n",
    "    input_layer = Input(shape=(None, None, image_features))\n",
    "    \n",
    "    c1 = Conv2D(\n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(input_layer)\n",
    "    c2 = Conv2D(\n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(c1)\n",
    "    return input_layer, c2\n",
    "\n",
    "def res_block(p_layer, f=128):\n",
    "    l1 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        dilation_rate=2,\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(p_layer)\n",
    "    l1 = BatchNormalization()(l1)\n",
    "    l2 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        dilation_rate=2,\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(l1)\n",
    "    l2 = BatchNormalization()(l2)\n",
    "    l3 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        dilation_rate=2,\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(l2)\n",
    "    l3 = BatchNormalization()(l3)\n",
    "    \n",
    "    a = Add()([p_layer, l3])\n",
    "    res = Activation('relu')(a)\n",
    "    return res\n",
    "    \n",
    "def branch_block(parent_layer, f=128):\n",
    "    m1 = MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "    )(parent_layer)\n",
    "    c3 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        activation='relu',\n",
    "        padding='same', \n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(m1)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    c4 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        activation='relu',\n",
    "        padding='same', \n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(c3)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    return c4\n",
    "\n",
    "def downsample(p_layer, f=128, scale=2):\n",
    "    cd1 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        strides=scale,\n",
    "        activation='relu',\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(p_layer)\n",
    "    cd1 = BatchNormalization()(cd1)\n",
    "    c5 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(cd1)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    c6 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(c5)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    return c6\n",
    "\n",
    "def stack_block(main_layer, parent_layer, f=128, scale=2):\n",
    "    b1 = branch_block(parent_layer=parent_layer, f=f)\n",
    "    b2 = downsample(p_layer=main_layer, f=f, scale=scale)\n",
    "    a1 = Add()([b1, b2])\n",
    "    res_b = res_block(p_layer=a1, f=f)\n",
    "    return res_b\n",
    "\n",
    "def stack_mid(parent):\n",
    "    f = int_shape(parent)[-1]\n",
    "    r1 = res_block(p_layer=parent, f=f)\n",
    "    r2 = res_block(p_layer=r1, f=f)\n",
    "    r3 = res_block(p_layer=r2, f=f)\n",
    "    \n",
    "    d1 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        dilation_rate=1,\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(r3)\n",
    "    d1 = BatchNormalization()(d1)\n",
    "    d2 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        dilation_rate=2,\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(r3)\n",
    "    d2 = BatchNormalization()(d2)\n",
    "    d3 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        dilation_rate=4,\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(r3)\n",
    "    d3 = BatchNormalization()(d3)\n",
    "    d4 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        dilation_rate=8,\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(r3)\n",
    "    d4 = BatchNormalization()(d4)\n",
    "    \n",
    "    c = Concatenate(axis=-1)([d1, d2, d3, d4])\n",
    "    return c\n",
    "\n",
    "def up_merge(xx_lyr, yy_lyr, f=128):\n",
    "    c11 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(xx_lyr)\n",
    "    c11 = BatchNormalization()(c11)\n",
    "\n",
    "    c12 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(c11)\n",
    "    c12 = BatchNormalization()(c12)\n",
    "    \n",
    "    upl = UpSampling2D(size=2, interpolation='bilinear')(c12)\n",
    "    \n",
    "    cat = Concatenate(axis=-1)([upl, yy_lyr])\n",
    "    return cat\n",
    "\n",
    "def final_blk(xx_lyr, f=128, count=4):\n",
    "    c11 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(xx_lyr)\n",
    "    c11 = BatchNormalization()(c11)\n",
    "\n",
    "    c12 = Conv2D(\n",
    "        filters=f, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(c11)\n",
    "    c12 = BatchNormalization()(c12)\n",
    "    \n",
    "    cx = Conv2D(\n",
    "        filters=count, \n",
    "        kernel_size=(1, 1), \n",
    "        padding='same', \n",
    "        activation='softmax',\n",
    "        data_format='channels_last',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "    )(c11)\n",
    "    return cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = make_features(input_shape=(512, 512, image_features), f=64)\n",
    "lyr1 = stack_block(main_layer=xx, parent_layer=yy, f=128, scale=2)\n",
    "lyr2 = stack_block(main_layer=xx, parent_layer=lyr1, f=256, scale=4)\n",
    "lyr3 = stack_block(main_layer=xx, parent_layer=lyr2, f=512, scale=8)\n",
    "mid = stack_mid(parent=lyr3)\n",
    "lyr4 = up_merge(xx_lyr=mid, yy_lyr=lyr2, f=512)\n",
    "lyr5 = up_merge(xx_lyr=lyr4, yy_lyr=lyr1, f=256)\n",
    "lyr6 = up_merge(xx_lyr=lyr5, yy_lyr=yy, f=128)\n",
    "zz = final_blk(xx_lyr=lyr6, f=64, count=class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnet_classifier = Model(inputs=xx, outputs=zz)\n",
    "xnet_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# xnet_classifier = xnet(\n",
    "#     input_shape=(None, None, image_features),\n",
    "#     num_classes=class_count,\n",
    "#     activation=\"relu\",\n",
    "#     use_batch_norm=True,\n",
    "#     upsample_mode=\"deconv\",  # 'deconv' or 'simple'\n",
    "#     dropout=droprate,\n",
    "#     dropout_change_per_layer=0.0,\n",
    "#     dropout_type=\"standard\",\n",
    "#     use_dropout_on_upsampling=False,\n",
    "#     use_attention=True,\n",
    "#     filters=64,\n",
    "#     num_layers=4,\n",
    "#     output_activation=\"softmax\",\n",
    "# )\n",
    "# xnet_classifier.summary(line_length=116)\n",
    "# plot_model(xnet_classifier, to_file=mplot, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_val_loss = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n",
    "# es_val_accu = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.001)\n",
    "mc_val_accu = ModelCheckpoint(str(model_max_accuracy.absolute()), monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "mc_val_loss = ModelCheckpoint(str(model_min_loss.absolute()), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "tb = TensorBoard(\n",
    "    log_dir=log_d, \n",
    "    histogram_freq=1, \n",
    "    write_graph=True, \n",
    "    write_images=True,\n",
    "    update_freq='batch', \n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "# Load Pre Trained Weights if any\n",
    "# if model_max_accuracy.is_file():\n",
    "#     xnet_classifier.load_weights(str(model_max_accuracy))\n",
    "\n",
    "xnet_classifier.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4), \n",
    "    # loss='categorical_crossentropy', \n",
    "    loss=focal_tversky_loss(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = RasterDataGenerator( \n",
    "    map_dict=train_map,\n",
    "    channels=bands,\n",
    "    img_height=image_height,\n",
    "    img_width=image_width,\n",
    "    win_height=window_height,\n",
    "    win_width=window_width,\n",
    "    min_hoverlap=min_height_overlap,\n",
    "    min_woverlap=min_width_overlap,\n",
    "    cls_count=class_count,\n",
    "    boundless=boundless_flag,\n",
    "    # augs=geo_augs,\n",
    "    augs=None,\n",
    "    shuffle=data_shuffle,\n",
    "    batch_size=batchsize\n",
    ")\n",
    "valid_generator = RasterDataGenerator(\n",
    "    map_dict=valid_map,\n",
    "    channels=bands,\n",
    "    img_height=image_height,\n",
    "    img_width=image_width,\n",
    "    win_height=window_height,\n",
    "    win_width=window_width,\n",
    "    min_hoverlap=1,\n",
    "    min_woverlap=1,\n",
    "    cls_count=class_count,\n",
    "    augs=None,\n",
    "    boundless=boundless_flag,\n",
    "    shuffle=data_shuffle,\n",
    "    batch_size=batchsize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "xnet_classifier.fit_generator(\n",
    "    generator=train_generator, \n",
    "    epochs=50, \n",
    "    validation_data=valid_generator,\n",
    "    use_multiprocessing=False,\n",
    "    callbacks=[\n",
    "        tb,\n",
    "        es_val_loss,\n",
    "        mc_val_loss,\n",
    "#         es_val_accu,\n",
    "        mc_val_accu,\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
