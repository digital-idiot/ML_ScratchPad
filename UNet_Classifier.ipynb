{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from pathlib import Path\n",
    "from math import floor, ceil\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from keras.models import Model\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "# from keras.layers import LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from rasterio import windows as rio_windows\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers import UpSampling2D, concatenate\n",
    "from keras.layers import Conv2D, BatchNormalization\n",
    "from keras.layers import MaxPooling2D, AveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "droprate = 0.1\n",
    "image_height = 6000\n",
    "image_width = 6000\n",
    "window_height = 512\n",
    "window_width = 512\n",
    "min_height_overlap = 64\n",
    "min_width_overlap = 64\n",
    "boundless_flag = True\n",
    "class_count = 6\n",
    "data_shuffle = True\n",
    "batchsize=2\n",
    "bands = (1, 2, 3, 4, 5)\n",
    "image_features = len(bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_augs = (\n",
    "    (lambda m : m),\n",
    "    partial(np.rot90, k=1, axes=(1, 2)),\n",
    "    partial(np.rot90, k=2, axes=(1, 2)),\n",
    "    partial(np.rot90, k=3, axes=(1, 2)),\n",
    "    partial(np.flip, axis=1),\n",
    "    partial(np.flip, axis=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data\n",
    "config_dir = Path('Configs')\n",
    "train_config = config_dir / 'Train_Map.json'\n",
    "valid_config = config_dir / 'Validation_Map.json'\n",
    "test_config = config_dir / 'Test_Map.json'\n",
    "model_dir = Path(\"Models\")\n",
    "mplot = model_dir / \"Model_Plot.png\"\n",
    "model_max_accuracy = model_dir / 'Model_MaxAccuracy.h5' \n",
    "model_min_loss = model_dir / 'Model_MinLoss.h5'\n",
    "log_d = Path('Logs')\n",
    "\n",
    "with open(train_config.as_posix(), 'r') as tm:\n",
    "    train_map = json.load(tm)\n",
    "\n",
    "with open(valid_config.as_posix(), 'r') as tm:\n",
    "    valid_map = json.load(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_windows(img_height, img_width, win_height, win_width, min_hoverlap, min_woverlap, boundless=False):\n",
    "    hc = ceil((img_height - min_hoverlap) / (win_height - min_hoverlap))\n",
    "    wc = ceil((img_width - min_woverlap) / (win_width - min_woverlap))\n",
    "    \n",
    "    \n",
    "    h_overlap = ((hc * win_height) - img_height) // (hc - 1)\n",
    "    w_overlap = ((wc * win_height) - img_width) // (wc - 1)\n",
    "    \n",
    "    \n",
    "    hslack_res = ((hc * win_height) - img_height) % (hc - 1)\n",
    "    wslack_res = ((wc * win_width) - img_width) % (wc - 1)\n",
    "    \n",
    "    dh = win_height - h_overlap\n",
    "    dw = win_width - w_overlap\n",
    "    \n",
    "    row_offsets = np.arange(0, (img_height-h_overlap), dh)\n",
    "    col_offsets = np.arange(0, (img_width-w_overlap), dw)\n",
    "    \n",
    "    if hslack_res > 0:\n",
    "        row_offsets[-hslack_res:] -= np.arange(1, (hslack_res + 1), 1)\n",
    "    if wslack_res > 0:\n",
    "        col_offsets[-wslack_res:] -= np.arange(1, (wslack_res + 1), 1)\n",
    "    \n",
    "    row_offsets = row_offsets.tolist()\n",
    "    col_offsets = col_offsets.tolist()\n",
    "    \n",
    "    offsets = product(col_offsets, row_offsets)\n",
    "    \n",
    "    indices = product(range(len(col_offsets)), range(len(row_offsets)))\n",
    "    \n",
    "    big_window = rio_windows.Window(col_off=0, row_off=0, width=img_width, height=img_height)\n",
    "    \n",
    "    for index, (col_off, row_off) in zip(indices, offsets):\n",
    "        window = rio_windows.Window(\n",
    "            col_off=col_off,\n",
    "            row_off=row_off,\n",
    "            width=win_width,\n",
    "            height=win_height\n",
    "        )\n",
    "        if boundless:\n",
    "            yield index, window\n",
    "        else:\n",
    "            yield index, window.intersection(big_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterDataGenerator(Sequence):\n",
    "    def __init__(\n",
    "        self,  \n",
    "        map_dict,\n",
    "        channels,\n",
    "        img_height,\n",
    "        img_width,\n",
    "        win_height,\n",
    "        win_width,\n",
    "        min_hoverlap,\n",
    "        min_woverlap,\n",
    "        cls_count,\n",
    "        boundless=False,\n",
    "        shuffle=True,\n",
    "        batch_size=1,\n",
    "    ):\n",
    "        assert isinstance(map_dict, dict), 'Invalid type for parameter <map_dict>, expected type `dict`!'\n",
    "        assert all([set(map_dict[k].keys()) == {'IMAGE', 'LABEL'} for k in map_dict.keys()]), \"Invalid map <dict_map>, Key Mismatch!\"\n",
    "        \n",
    "        couples =  [(Path(couple['IMAGE']).as_posix(), Path(couple['LABEL']).as_posix()) for couple in map_dict.values()]\n",
    "        \n",
    "        windows = list(\n",
    "            generate_windows(\n",
    "                img_height=img_height,\n",
    "                img_width=img_width,\n",
    "                win_height=win_height,\n",
    "                win_width=win_width,\n",
    "                min_hoverlap=min_hoverlap,\n",
    "                min_woverlap=min_woverlap,\n",
    "                boundless=boundless\n",
    "            )\n",
    "        )\n",
    "        dat = list(product(couples, windows))\n",
    "        if shuffle:\n",
    "            random.shuffle(dat)\n",
    "        self.data = dat\n",
    "        self.channels = channels\n",
    "        self.class_count = cls_count\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        current_batch = self.data[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        islices = list()\n",
    "        lslices = list()\n",
    "        for (im, lb), (_, w) in current_batch:\n",
    "            with rio.open(im, 'r') as isrc:\n",
    "                islice = isrc.read(indexes=self.channels, window=w, boundless=boundless_flag, masked=False)\n",
    "                islice = np.moveaxis(a=islice, source=0, destination=-1)\n",
    "                islices.append(islice / 255.0)\n",
    "            with rio.open(lb, 'r') as lsrc:\n",
    "                lslice = lsrc.read(window=w, boundless=boundless_flag, masked=False)\n",
    "                lslice = np.moveaxis(a=lslice, source=0, destination=-1)\n",
    "                lslice =to_categorical(\n",
    "                    y=(lslice-1), \n",
    "                    num_classes=self.class_count\n",
    "                )\n",
    "                lslices.append(lslice)\n",
    "        ibatch = np.stack(islices, axis=0)\n",
    "        lbatch = np.stack(lslices, axis=0)\n",
    "        return ibatch, lbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin\n",
    "input_layer = Input(\n",
    "    shape=(None, None, image_features), \n",
    "    name='input_layer'\n",
    ")\n",
    "\n",
    "# Part e1\n",
    "convolve_layer_1a = Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl1a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_1a'\n",
    ")(input_layer)\n",
    "\n",
    "norm_layer_1a = BatchNormalization(name='norm_layer_1a')(convolve_layer_1a)\n",
    "\n",
    "convolve_layer_1b = Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl1b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_1b'\n",
    ")(norm_layer_1a)\n",
    "\n",
    "norm_layer_1b = BatchNormalization(name='norm_layer_1b')(convolve_layer_1b)\n",
    "\n",
    "drop_layer_1 = Dropout(droprate, name='drop_layer_1')(norm_layer_1b)\n",
    "\n",
    "pooling_layer_1 = MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    name='pooling_layer_1'\n",
    ")(drop_layer_1)\n",
    "\n",
    "\n",
    "# Part e2\n",
    "convolve_layer_2a = Conv2D(\n",
    "    filters=128, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl2a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_2a'\n",
    ")(pooling_layer_1)\n",
    "\n",
    "norm_layer_2a = BatchNormalization(name='norm_layer_2a')(convolve_layer_2a)\n",
    "\n",
    "convolve_layer_2b = Conv2D(\n",
    "    filters=128, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl2b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_2b'\n",
    ")(norm_layer_2a)\n",
    "\n",
    "norm_layer_2b = BatchNormalization(name='norm_layer_2b')(convolve_layer_2b)\n",
    "\n",
    "drop_layer_2 = Dropout(droprate, name='drop_layer_2')(norm_layer_2b)\n",
    "\n",
    "pooling_layer_2 = MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    name='pooling_layer_2'\n",
    ")(drop_layer_2)\n",
    "\n",
    "\n",
    "# Part e3\n",
    "convolve_layer_3a = Conv2D(\n",
    "    filters=256, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl3a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_3a'\n",
    ")(pooling_layer_2)\n",
    "\n",
    "norm_layer_3a = BatchNormalization(name='norm_layer_3a')(convolve_layer_3a)\n",
    "\n",
    "convolve_layer_3b = Conv2D(\n",
    "    filters=256, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl3b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_3b'\n",
    ")(norm_layer_3a)\n",
    "\n",
    "norm_layer_3b = BatchNormalization(name='norm_layer_3b')(convolve_layer_3b)\n",
    "\n",
    "drop_layer_3 = Dropout(droprate, name='drop_layer_3')(norm_layer_3b)\n",
    "\n",
    "pooling_layer_3 = MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    name='pooling_layer_3'\n",
    ")(drop_layer_3)\n",
    "\n",
    "# Part e4\n",
    "convolve_layer_4a = Conv2D(\n",
    "    filters=512, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl4a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_4a'\n",
    ")(pooling_layer_3)\n",
    "\n",
    "norm_layer_4a = BatchNormalization(name='norm_layer_4a')(convolve_layer_4a)\n",
    "\n",
    "convolve_layer_4b = Conv2D(\n",
    "    filters=512, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl4b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    # kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_4b'\n",
    ")(norm_layer_4a)\n",
    "\n",
    "norm_layer_4b = BatchNormalization(name='norm_layer_4b')(convolve_layer_4b)\n",
    "\n",
    "drop_layer_4 = Dropout(droprate, name='drop_layer_4')(norm_layer_4b)\n",
    "\n",
    "pooling_layer_4 = MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    name='pooling_layer_4'\n",
    ")(drop_layer_4)\n",
    "\n",
    "# Part Center\n",
    "convolve_layer_xa = Conv2D(\n",
    "    filters=1024, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_clxa'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_xa'\n",
    ")(pooling_layer_4)\n",
    "\n",
    "norm_layer_xa = BatchNormalization(name='norm_layer_xa')(convolve_layer_xa)\n",
    "\n",
    "convolve_layer_xb = Conv2D(\n",
    "    filters=1024, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_clxb'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_xb'\n",
    ")(norm_layer_xa)\n",
    "\n",
    "norm_layer_xb = BatchNormalization(name='norm_layer_xb')(convolve_layer_xb)\n",
    "\n",
    "drop_layer_x = Dropout(droprate, name='drop_layer_x')(norm_layer_xb)\n",
    "\n",
    "\n",
    "# Part d1\n",
    "upsample_layer_1 = Conv2D(\n",
    "    filters=512, \n",
    "    kernel_size=(2, 2), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_ul1'), \n",
    "    activation='relu',\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='upsample_layer_1'\n",
    ")(\n",
    "    UpSampling2D(\n",
    "        size=(2, 2),\n",
    "        data_format='channels_last',\n",
    "    )(drop_layer_x)\n",
    ")\n",
    "\n",
    "merge_layer_1 = concatenate(\n",
    "    [\n",
    "        drop_layer_4, \n",
    "        upsample_layer_1\n",
    "    ],\n",
    "    axis = 3, \n",
    "    name='merge_layer_1'\n",
    ")\n",
    "\n",
    "convolve_layer_5a = convolve_layer = Conv2D(\n",
    "    filters=512, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl5a'),\n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_5a'\n",
    ")(merge_layer_1)\n",
    "\n",
    "convolve_layer_5b = convolve_layer = Conv2D(\n",
    "    filters=512, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl5b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_5b'\n",
    ")(convolve_layer_5a)\n",
    "\n",
    "# Part d2\n",
    "upsample_layer_2 = Conv2D(\n",
    "    filters=256, \n",
    "    kernel_size=(2, 2), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_ul2'),\n",
    "    activation='relu',\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='upsample_layer_2'\n",
    ")(\n",
    "    UpSampling2D(\n",
    "        size=(2, 2),\n",
    "        data_format='channels_last',\n",
    "    )(convolve_layer_5b)\n",
    ")\n",
    "\n",
    "merge_layer_2 = concatenate(\n",
    "    [\n",
    "        drop_layer_3, \n",
    "        upsample_layer_2\n",
    "    ],\n",
    "    axis = 3, \n",
    "    name='merge_layer_2'\n",
    ")\n",
    "\n",
    "convolve_layer_6a = convolve_layer = Conv2D(\n",
    "    filters=256, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl6a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_6a'\n",
    ")(merge_layer_2)\n",
    "\n",
    "convolve_layer_6b = convolve_layer = Conv2D(\n",
    "    filters=256, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl6b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_6b'\n",
    ")(convolve_layer_6a)\n",
    "\n",
    "# Part d3\n",
    "upsample_layer_3 = Conv2D(\n",
    "    filters=128, \n",
    "    kernel_size=(2, 2), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_ul3'), \n",
    "    activation='relu',\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='upsample_layer_3'\n",
    ")(\n",
    "    UpSampling2D(\n",
    "        size=(2, 2),\n",
    "        data_format='channels_last',\n",
    "    )(convolve_layer_6b)\n",
    ")\n",
    "\n",
    "merge_layer_3 = concatenate(\n",
    "    [\n",
    "        drop_layer_2, \n",
    "        upsample_layer_3\n",
    "    ],\n",
    "    axis = 3, \n",
    "    name='merge_layer_3'\n",
    ")\n",
    "\n",
    "convolve_layer_7a = convolve_layer = Conv2D(\n",
    "    filters=128, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl7a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_7a'\n",
    ")(merge_layer_3)\n",
    "\n",
    "convolve_layer_7b = convolve_layer = Conv2D(\n",
    "    filters=128, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl5b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_7b'\n",
    ")(convolve_layer_7a)\n",
    "\n",
    "# Part d4\n",
    "upsample_layer_4 = Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=(2, 2), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_ul4'), \n",
    "    activation='relu',\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='upsample_layer_4'\n",
    ")(\n",
    "    UpSampling2D(\n",
    "        size=(2, 2),\n",
    "        data_format='channels_last',\n",
    "    )(convolve_layer_7b)\n",
    ")\n",
    "\n",
    "merge_layer_4 = concatenate(\n",
    "    [\n",
    "        drop_layer_1, \n",
    "        upsample_layer_4\n",
    "    ],\n",
    "    axis = 3, \n",
    "    name='merge_layer_4'\n",
    ")\n",
    "\n",
    "convolve_layer_8a = convolve_layer = Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl8a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_8a'\n",
    ")(merge_layer_4)\n",
    "\n",
    "convolve_layer_8b = convolve_layer = Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl8b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_8b'\n",
    ")(convolve_layer_8a)\n",
    "\n",
    "output_layer = Conv2D(\n",
    "    filters=class_count, \n",
    "    kernel_size=(1, 1), \n",
    "    activation='softmax', \n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='output_layer'\n",
    ")(convolve_layer_8b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unet_classifier = Model(inputs=input_layer, outputs=output_layer)\n",
    "unet_classifier.summary()\n",
    "# plot_model(unet_classifier, to_file=mplot, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "es_val_loss = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n",
    "# es_val_accu = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.001)\n",
    "mc_val_accu = ModelCheckpoint(str(model_max_accuracy.absolute()), monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "mc_val_loss = ModelCheckpoint(str(model_min_loss.absolute()), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "tb = TensorBoard(\n",
    "    log_dir=log_d, \n",
    "    histogram_freq=1, \n",
    "    write_graph=True, \n",
    "    write_images=True,\n",
    "    update_freq='batch', \n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "\n",
    "unet_classifier.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = RasterDataGenerator( \n",
    "    map_dict=train_map,\n",
    "    channels=bands,\n",
    "    img_height=image_height,\n",
    "    img_width=image_width,\n",
    "    win_height=window_height,\n",
    "    win_width=window_width,\n",
    "    min_hoverlap=min_height_overlap,\n",
    "    min_woverlap=min_width_overlap,\n",
    "    cls_count=class_count,\n",
    "    boundless=boundless_flag,\n",
    "    shuffle=data_shuffle,\n",
    "    batch_size=batchsize\n",
    ")\n",
    "valid_generator = RasterDataGenerator(\n",
    "    map_dict=valid_map,\n",
    "    channels=bands,\n",
    "    img_height=image_height,\n",
    "    img_width=image_width,\n",
    "    win_height=window_height,\n",
    "    win_width=window_width,\n",
    "    min_hoverlap=min_height_overlap,\n",
    "    min_woverlap=min_width_overlap,\n",
    "    cls_count=class_count,\n",
    "    boundless=boundless_flag,\n",
    "    shuffle=data_shuffle,\n",
    "    batch_size=batchsize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "unet_classifier.fit_generator(\n",
    "    generator=train_generator, \n",
    "    epochs=50, \n",
    "    validation_data=valid_generator,\n",
    "    use_multiprocessing=False,\n",
    "    callbacks=[\n",
    "        tb,\n",
    "        es_val_loss,\n",
    "        mc_val_loss,\n",
    "#         es_val_accu,\n",
    "        mc_val_accu,\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
