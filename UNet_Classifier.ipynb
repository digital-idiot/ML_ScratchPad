{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from pathlib import Path\n",
    "from math import floor, ceil\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from keras.models import Model\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "# from keras.layers import LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from rasterio import windows as rio_windows\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers import UpSampling2D, concatenate\n",
    "from keras.layers import Conv2D, BatchNormalization\n",
    "from keras.layers import MaxPooling2D, AveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "droprate = 0.1\n",
    "image_height = 6000\n",
    "image_width = 6000\n",
    "window_height = 512\n",
    "window_width = 512\n",
    "min_height_overlap = 64\n",
    "min_width_overlap = 64\n",
    "boundless_flag = True\n",
    "class_count = 6\n",
    "data_shuffle = True\n",
    "batchsize=2\n",
    "bands = (1, 2, 3, 4, 5)\n",
    "image_features = len(bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_augs = (\n",
    "    (lambda m : m),\n",
    "    partial(np.rot90, k=1, axes=(1, 2)),\n",
    "    partial(np.rot90, k=2, axes=(1, 2)),\n",
    "    partial(np.rot90, k=3, axes=(1, 2)),\n",
    "    partial(np.flip, axis=1),\n",
    "    partial(np.flip, axis=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data\n",
    "config_dir = Path('Configs')\n",
    "train_config = config_dir / 'Train_Map.json'\n",
    "valid_config = config_dir / 'Validation_Map.json'\n",
    "test_config = config_dir / 'Test_Map.json'\n",
    "model_dir = Path(\"Models\")\n",
    "mplot = model_dir / \"Model_Plot.png\"\n",
    "model_max_accuracy = model_dir / 'Model_MaxAccuracy.h5' \n",
    "model_min_loss = model_dir / 'Model_MinLoss.h5'\n",
    "log_d = Path('Logs')\n",
    "\n",
    "with open(train_config.as_posix(), 'r') as tm:\n",
    "    train_map = json.load(tm)\n",
    "\n",
    "with open(valid_config.as_posix(), 'r') as tm:\n",
    "    valid_map = json.load(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_windows(img_height, img_width, win_height, win_width, min_hoverlap, min_woverlap, boundless=False):\n",
    "    hc = ceil((img_height - min_hoverlap) / (win_height - min_hoverlap))\n",
    "    wc = ceil((img_width - min_woverlap) / (win_width - min_woverlap))\n",
    "    \n",
    "    \n",
    "    h_overlap = ((hc * win_height) - img_height) // (hc - 1)\n",
    "    w_overlap = ((wc * win_height) - img_width) // (wc - 1)\n",
    "    \n",
    "    \n",
    "    hslack_res = ((hc * win_height) - img_height) % (hc - 1)\n",
    "    wslack_res = ((wc * win_width) - img_width) % (wc - 1)\n",
    "    \n",
    "    dh = win_height - h_overlap\n",
    "    dw = win_width - w_overlap\n",
    "    \n",
    "    row_offsets = np.arange(0, (img_height-h_overlap), dh)\n",
    "    col_offsets = np.arange(0, (img_width-w_overlap), dw)\n",
    "    \n",
    "    if hslack_res > 0:\n",
    "        row_offsets[-hslack_res:] -= np.arange(1, (hslack_res + 1), 1)\n",
    "    if wslack_res > 0:\n",
    "        col_offsets[-wslack_res:] -= np.arange(1, (wslack_res + 1), 1)\n",
    "    \n",
    "    row_offsets = row_offsets.tolist()\n",
    "    col_offsets = col_offsets.tolist()\n",
    "    \n",
    "    offsets = product(col_offsets, row_offsets)\n",
    "    \n",
    "    indices = product(range(len(col_offsets)), range(len(row_offsets)))\n",
    "    \n",
    "    big_window = rio_windows.Window(col_off=0, row_off=0, width=img_width, height=img_height)\n",
    "    \n",
    "    for index, (col_off, row_off) in zip(indices, offsets):\n",
    "        window = rio_windows.Window(\n",
    "            col_off=col_off,\n",
    "            row_off=row_off,\n",
    "            width=win_width,\n",
    "            height=win_height\n",
    "        )\n",
    "        if boundless:\n",
    "            yield index, window\n",
    "        else:\n",
    "            yield index, window.intersection(big_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterDataGenerator(Sequence):\n",
    "    def __init__(\n",
    "        self,  \n",
    "        map_dict,\n",
    "        channels,\n",
    "        img_height,\n",
    "        img_width,\n",
    "        win_height,\n",
    "        win_width,\n",
    "        min_hoverlap,\n",
    "        min_woverlap,\n",
    "        cls_count,\n",
    "        boundless=False,\n",
    "        shuffle=True,\n",
    "        batch_size=1,\n",
    "    ):\n",
    "        assert isinstance(map_dict, dict), 'Invalid type for parameter <map_dict>, expected type `dict`!'\n",
    "        assert all([set(map_dict[k].keys()) == {'IMAGE', 'LABEL'} for k in map_dict.keys()]), \"Invalid map <dict_map>, Key Mismatch!\"\n",
    "        \n",
    "        couples =  [(Path(couple['IMAGE']).as_posix(), Path(couple['LABEL']).as_posix()) for couple in map_dict.values()]\n",
    "        \n",
    "        windows = list(\n",
    "            generate_windows(\n",
    "                img_height=img_height,\n",
    "                img_width=img_width,\n",
    "                win_height=win_height,\n",
    "                win_width=win_width,\n",
    "                min_hoverlap=min_hoverlap,\n",
    "                min_woverlap=min_woverlap,\n",
    "                boundless=boundless\n",
    "            )\n",
    "        )\n",
    "        dat = list(product(couples, windows))\n",
    "        if shuffle:\n",
    "            random.shuffle(dat)\n",
    "        self.data = dat\n",
    "        self.channels = channels\n",
    "        self.class_count = cls_count\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        current_batch = self.data[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        islices = list()\n",
    "        lslices = list()\n",
    "        for (im, lb), (_, w) in current_batch:\n",
    "            with rio.open(im, 'r') as isrc:\n",
    "                islice = isrc.read(indexes=self.channels, window=w, boundless=boundless_flag, masked=False)\n",
    "                islice = np.moveaxis(a=islice, source=0, destination=-1)\n",
    "                islices.append(islice / 255.0)\n",
    "            with rio.open(lb, 'r') as lsrc:\n",
    "                lslice = lsrc.read(window=w, boundless=boundless_flag, masked=False)\n",
    "                lslice = np.moveaxis(a=lslice, source=0, destination=-1)\n",
    "                lslice =to_categorical(\n",
    "                    y=(lslice-1), \n",
    "                    num_classes=self.class_count\n",
    "                )\n",
    "                lslices.append(lslice)\n",
    "        ibatch = np.stack(islices, axis=0)\n",
    "        lbatch = np.stack(lslices, axis=0)\n",
    "        return ibatch, lbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin\n",
    "input_layer = Input(\n",
    "    shape=(None, None, image_features), \n",
    "    name='input_layer'\n",
    ")\n",
    "\n",
    "# Part e1\n",
    "convolve_layer_1a = Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl1a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_1a'\n",
    ")(input_layer)\n",
    "\n",
    "norm_layer_1a = BatchNormalization(name='norm_layer_1a')(convolve_layer_1a)\n",
    "\n",
    "convolve_layer_1b = Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl1b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_1b'\n",
    ")(norm_layer_1a)\n",
    "\n",
    "norm_layer_1b = BatchNormalization(name='norm_layer_1b')(convolve_layer_1b)\n",
    "\n",
    "drop_layer_1 = Dropout(droprate, name='drop_layer_1')(norm_layer_1b)\n",
    "\n",
    "pooling_layer_1 = MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    name='pooling_layer_1'\n",
    ")(drop_layer_1)\n",
    "\n",
    "\n",
    "# Part e2\n",
    "convolve_layer_2a = Conv2D(\n",
    "    filters=128, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl2a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_2a'\n",
    ")(pooling_layer_1)\n",
    "\n",
    "norm_layer_2a = BatchNormalization(name='norm_layer_2a')(convolve_layer_2a)\n",
    "\n",
    "convolve_layer_2b = Conv2D(\n",
    "    filters=128, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl2b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_2b'\n",
    ")(norm_layer_2a)\n",
    "\n",
    "norm_layer_2b = BatchNormalization(name='norm_layer_2b')(convolve_layer_2b)\n",
    "\n",
    "drop_layer_2 = Dropout(droprate, name='drop_layer_2')(norm_layer_2b)\n",
    "\n",
    "pooling_layer_2 = MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    name='pooling_layer_2'\n",
    ")(drop_layer_2)\n",
    "\n",
    "\n",
    "# Part e3\n",
    "convolve_layer_3a = Conv2D(\n",
    "    filters=256, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl3a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_3a'\n",
    ")(pooling_layer_2)\n",
    "\n",
    "norm_layer_3a = BatchNormalization(name='norm_layer_3a')(convolve_layer_3a)\n",
    "\n",
    "convolve_layer_3b = Conv2D(\n",
    "    filters=256, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl3b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_3b'\n",
    ")(norm_layer_3a)\n",
    "\n",
    "norm_layer_3b = BatchNormalization(name='norm_layer_3b')(convolve_layer_3b)\n",
    "\n",
    "drop_layer_3 = Dropout(droprate, name='drop_layer_3')(norm_layer_3b)\n",
    "\n",
    "pooling_layer_3 = MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    name='pooling_layer_3'\n",
    ")(drop_layer_3)\n",
    "\n",
    "# Part e4\n",
    "convolve_layer_4a = Conv2D(\n",
    "    filters=512, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl4a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_4a'\n",
    ")(pooling_layer_3)\n",
    "\n",
    "norm_layer_4a = BatchNormalization(name='norm_layer_4a')(convolve_layer_4a)\n",
    "\n",
    "convolve_layer_4b = Conv2D(\n",
    "    filters=512, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl4b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    # kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_4b'\n",
    ")(norm_layer_4a)\n",
    "\n",
    "norm_layer_4b = BatchNormalization(name='norm_layer_4b')(convolve_layer_4b)\n",
    "\n",
    "drop_layer_4 = Dropout(droprate, name='drop_layer_4')(norm_layer_4b)\n",
    "\n",
    "pooling_layer_4 = MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    name='pooling_layer_4'\n",
    ")(drop_layer_4)\n",
    "\n",
    "# Part Center\n",
    "convolve_layer_xa = Conv2D(\n",
    "    filters=1024, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_clxa'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_xa'\n",
    ")(pooling_layer_4)\n",
    "\n",
    "norm_layer_xa = BatchNormalization(name='norm_layer_xa')(convolve_layer_xa)\n",
    "\n",
    "convolve_layer_xb = Conv2D(\n",
    "    filters=1024, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_clxb'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_xb'\n",
    ")(norm_layer_xa)\n",
    "\n",
    "norm_layer_xb = BatchNormalization(name='norm_layer_xb')(convolve_layer_xb)\n",
    "\n",
    "drop_layer_x = Dropout(droprate, name='drop_layer_x')(norm_layer_xb)\n",
    "\n",
    "\n",
    "# Part d1\n",
    "upsample_layer_1 = Conv2D(\n",
    "    filters=512, \n",
    "    kernel_size=(2, 2), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_ul1'), \n",
    "    activation='relu',\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='upsample_layer_1'\n",
    ")(\n",
    "    UpSampling2D(\n",
    "        size=(2, 2),\n",
    "        data_format='channels_last',\n",
    "    )(drop_layer_x)\n",
    ")\n",
    "\n",
    "merge_layer_1 = concatenate(\n",
    "    [\n",
    "        drop_layer_4, \n",
    "        upsample_layer_1\n",
    "    ],\n",
    "    axis = 3, \n",
    "    name='merge_layer_1'\n",
    ")\n",
    "\n",
    "convolve_layer_5a = convolve_layer = Conv2D(\n",
    "    filters=512, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl5a'),\n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_5a'\n",
    ")(merge_layer_1)\n",
    "\n",
    "convolve_layer_5b = convolve_layer = Conv2D(\n",
    "    filters=512, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl5b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_5b'\n",
    ")(convolve_layer_5a)\n",
    "\n",
    "# Part d2\n",
    "upsample_layer_2 = Conv2D(\n",
    "    filters=256, \n",
    "    kernel_size=(2, 2), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_ul2'),\n",
    "    activation='relu',\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='upsample_layer_2'\n",
    ")(\n",
    "    UpSampling2D(\n",
    "        size=(2, 2),\n",
    "        data_format='channels_last',\n",
    "    )(convolve_layer_5b)\n",
    ")\n",
    "\n",
    "merge_layer_2 = concatenate(\n",
    "    [\n",
    "        drop_layer_3, \n",
    "        upsample_layer_2\n",
    "    ],\n",
    "    axis = 3, \n",
    "    name='merge_layer_2'\n",
    ")\n",
    "\n",
    "convolve_layer_6a = convolve_layer = Conv2D(\n",
    "    filters=256, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl6a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_6a'\n",
    ")(merge_layer_2)\n",
    "\n",
    "convolve_layer_6b = convolve_layer = Conv2D(\n",
    "    filters=256, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl6b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_6b'\n",
    ")(convolve_layer_6a)\n",
    "\n",
    "# Part d3\n",
    "upsample_layer_3 = Conv2D(\n",
    "    filters=128, \n",
    "    kernel_size=(2, 2), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_ul3'), \n",
    "    activation='relu',\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='upsample_layer_3'\n",
    ")(\n",
    "    UpSampling2D(\n",
    "        size=(2, 2),\n",
    "        data_format='channels_last',\n",
    "    )(convolve_layer_6b)\n",
    ")\n",
    "\n",
    "merge_layer_3 = concatenate(\n",
    "    [\n",
    "        drop_layer_2, \n",
    "        upsample_layer_3\n",
    "    ],\n",
    "    axis = 3, \n",
    "    name='merge_layer_3'\n",
    ")\n",
    "\n",
    "convolve_layer_7a = convolve_layer = Conv2D(\n",
    "    filters=128, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl7a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_7a'\n",
    ")(merge_layer_3)\n",
    "\n",
    "convolve_layer_7b = convolve_layer = Conv2D(\n",
    "    filters=128, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl5b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_7b'\n",
    ")(convolve_layer_7a)\n",
    "\n",
    "# Part d4\n",
    "upsample_layer_4 = Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=(2, 2), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_ul4'), \n",
    "    activation='relu',\n",
    "    padding='same',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='upsample_layer_4'\n",
    ")(\n",
    "    UpSampling2D(\n",
    "        size=(2, 2),\n",
    "        data_format='channels_last',\n",
    "    )(convolve_layer_7b)\n",
    ")\n",
    "\n",
    "merge_layer_4 = concatenate(\n",
    "    [\n",
    "        drop_layer_1, \n",
    "        upsample_layer_4\n",
    "    ],\n",
    "    axis = 3, \n",
    "    name='merge_layer_4'\n",
    ")\n",
    "\n",
    "convolve_layer_8a = convolve_layer = Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl8a'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_8a'\n",
    ")(merge_layer_4)\n",
    "\n",
    "convolve_layer_8b = convolve_layer = Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=(3, 3), \n",
    "#     activation=LeakyReLU(alpha=0.1, name='act_cl8b'), \n",
    "    activation='relu',\n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='convolve_layer_8b'\n",
    ")(convolve_layer_8a)\n",
    "\n",
    "output_layer = Conv2D(\n",
    "    filters=class_count, \n",
    "    kernel_size=(1, 1), \n",
    "    activation='softmax', \n",
    "    padding='same', \n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    # kernel_regularizer=l2(0.01), \n",
    "    # bias_regularizer=l2(0.01),\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='output_layer'\n",
    ")(convolve_layer_8b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        (None, None, None, 5 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_1a (Conv2D)      (None, None, None, 6 2944        input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "norm_layer_1a (BatchNormalizati (None, None, None, 6 256         convolve_layer_1a[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_1b (Conv2D)      (None, None, None, 6 36928       norm_layer_1a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_layer_1b (BatchNormalizati (None, None, None, 6 256         convolve_layer_1b[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "drop_layer_1 (Dropout)          (None, None, None, 6 0           norm_layer_1b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pooling_layer_1 (MaxPooling2D)  (None, None, None, 6 0           drop_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_2a (Conv2D)      (None, None, None, 1 73856       pooling_layer_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_layer_2a (BatchNormalizati (None, None, None, 1 512         convolve_layer_2a[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_2b (Conv2D)      (None, None, None, 1 147584      norm_layer_2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_layer_2b (BatchNormalizati (None, None, None, 1 512         convolve_layer_2b[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "drop_layer_2 (Dropout)          (None, None, None, 1 0           norm_layer_2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pooling_layer_2 (MaxPooling2D)  (None, None, None, 1 0           drop_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_3a (Conv2D)      (None, None, None, 2 295168      pooling_layer_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_layer_3a (BatchNormalizati (None, None, None, 2 1024        convolve_layer_3a[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_3b (Conv2D)      (None, None, None, 2 590080      norm_layer_3a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_layer_3b (BatchNormalizati (None, None, None, 2 1024        convolve_layer_3b[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "drop_layer_3 (Dropout)          (None, None, None, 2 0           norm_layer_3b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pooling_layer_3 (MaxPooling2D)  (None, None, None, 2 0           drop_layer_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_4a (Conv2D)      (None, None, None, 5 1180160     pooling_layer_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_layer_4a (BatchNormalizati (None, None, None, 5 2048        convolve_layer_4a[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_4b (Conv2D)      (None, None, None, 5 2359808     norm_layer_4a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_layer_4b (BatchNormalizati (None, None, None, 5 2048        convolve_layer_4b[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "drop_layer_4 (Dropout)          (None, None, None, 5 0           norm_layer_4b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pooling_layer_4 (MaxPooling2D)  (None, None, None, 5 0           drop_layer_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_xa (Conv2D)      (None, None, None, 1 4719616     pooling_layer_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_layer_xa (BatchNormalizati (None, None, None, 1 4096        convolve_layer_xa[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_xb (Conv2D)      (None, None, None, 1 9438208     norm_layer_xa[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_layer_xb (BatchNormalizati (None, None, None, 1 4096        convolve_layer_xb[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "drop_layer_x (Dropout)          (None, None, None, 1 0           norm_layer_xb[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, None, None, 1 0           drop_layer_x[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "upsample_layer_1 (Conv2D)       (None, None, None, 5 2097664     up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_layer_1 (Concatenate)     (None, None, None, 1 0           drop_layer_4[0][0]               \n",
      "                                                                 upsample_layer_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_5a (Conv2D)      (None, None, None, 5 4719104     merge_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_5b (Conv2D)      (None, None, None, 5 2359808     convolve_layer_5a[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, None, None, 5 0           convolve_layer_5b[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "upsample_layer_2 (Conv2D)       (None, None, None, 2 524544      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_layer_2 (Concatenate)     (None, None, None, 5 0           drop_layer_3[0][0]               \n",
      "                                                                 upsample_layer_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_6a (Conv2D)      (None, None, None, 2 1179904     merge_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_6b (Conv2D)      (None, None, None, 2 590080      convolve_layer_6a[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, None, None, 2 0           convolve_layer_6b[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "upsample_layer_3 (Conv2D)       (None, None, None, 1 131200      up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_layer_3 (Concatenate)     (None, None, None, 2 0           drop_layer_2[0][0]               \n",
      "                                                                 upsample_layer_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_7a (Conv2D)      (None, None, None, 1 295040      merge_layer_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_7b (Conv2D)      (None, None, None, 1 147584      convolve_layer_7a[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, None, None, 1 0           convolve_layer_7b[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "upsample_layer_4 (Conv2D)       (None, None, None, 6 32832       up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_layer_4 (Concatenate)     (None, None, None, 1 0           drop_layer_1[0][0]               \n",
      "                                                                 upsample_layer_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_8a (Conv2D)      (None, None, None, 6 73792       merge_layer_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convolve_layer_8b (Conv2D)      (None, None, None, 6 36928       convolve_layer_8a[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Conv2D)           (None, None, None, 6 390         convolve_layer_8b[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 31,049,094\n",
      "Trainable params: 31,041,158\n",
      "Non-trainable params: 7,936\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet_classifier = Model(inputs=input_layer, outputs=output_layer)\n",
    "unet_classifier.summary()\n",
    "# plot_model(unet_classifier, to_file=mplot, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "es_val_loss = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n",
    "# es_val_accu = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.001)\n",
    "mc_val_accu = ModelCheckpoint(str(model_max_accuracy.absolute()), monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "mc_val_loss = ModelCheckpoint(str(model_min_loss.absolute()), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "tb = TensorBoard(\n",
    "    log_dir=log_d, \n",
    "    histogram_freq=1, \n",
    "    write_graph=True, \n",
    "    write_images=True,\n",
    "    update_freq='batch', \n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "\n",
    "unet_classifier.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = RasterDataGenerator( \n",
    "    map_dict=train_map,\n",
    "    channels=bands,\n",
    "    img_height=image_height,\n",
    "    img_width=image_width,\n",
    "    win_height=window_height,\n",
    "    win_width=window_width,\n",
    "    min_hoverlap=min_height_overlap,\n",
    "    min_woverlap=min_width_overlap,\n",
    "    cls_count=class_count,\n",
    "    boundless=boundless_flag,\n",
    "    shuffle=data_shuffle,\n",
    "    batch_size=batchsize\n",
    ")\n",
    "valid_generator = RasterDataGenerator(\n",
    "    map_dict=valid_map,\n",
    "    channels=bands,\n",
    "    img_height=image_height,\n",
    "    img_width=image_width,\n",
    "    win_height=window_height,\n",
    "    win_width=window_width,\n",
    "    min_hoverlap=min_height_overlap,\n",
    "    min_woverlap=min_width_overlap,\n",
    "    cls_count=class_count,\n",
    "    boundless=boundless_flag,\n",
    "    shuffle=data_shuffle,\n",
    "    batch_size=batchsize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node convolve_layer_1a/convolution (defined at /home/abhisek/.conda/envs/Py3Dev/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_keras_scratch_graph_14279]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-626d2c22389a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmc_val_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#         es_val_accu,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmc_val_accu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     ]\n",
      "\u001b[0;32m~/.conda/envs/Py3Dev/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Py3Dev/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Py3Dev/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Py3Dev/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Py3Dev/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Py3Dev/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Py3Dev/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Py3Dev/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Py3Dev/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/Py3Dev/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/.conda/envs/Py3Dev/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node convolve_layer_1a/convolution (defined at /home/abhisek/.conda/envs/Py3Dev/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_keras_scratch_graph_14279]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "unet_classifier.fit_generator(\n",
    "    generator=train_generator, \n",
    "    epochs=50, \n",
    "    validation_data=valid_generator,\n",
    "    use_multiprocessing=False,\n",
    "    callbacks=[\n",
    "        tb,\n",
    "        es_val_loss,\n",
    "        mc_val_loss,\n",
    "#         es_val_accu,\n",
    "        mc_val_accu,\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
